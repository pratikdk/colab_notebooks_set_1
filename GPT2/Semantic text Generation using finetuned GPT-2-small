{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Semantic text Generation using finetuned GPT-2-small","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eY0Q5d2vBUvf","colab_type":"text"},"source":["**Fine Tuning on GPT-2**\n","\n","Fine-Tuning GPT-2 Small for Generative Text: https://pmbaumgartner.github.io/blog/gpt2-jokes/\n","\n","How to Build OpenAI's GPT-2: \"The AI That's Too Dangerous to Release\": https://blog.floydhub.com/gpt2/\n","\n","Generating Fake Conversations by fine-tuning OpenAI's GPT-2 on data from Facebook Messenger: https://svilentodorov.xyz/blog/gpt-finetune\n","\n","Generative Text Scenarios - GPT-2: http://www.mattkenney.me/gpt-2/\n","\n","**ULM and Transfer Learning:**\n","\n","NLP's ImageNet moment has arrived: http://ruder.io/nlp-imagenet/\n","\n","Introducing state of the art text classification with universal language models: http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html\n","\n","The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning): http://jalammar.github.io/illustrated-bert/\n","\n","The Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/\n","\n","Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention): https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n","\n","[Paper] Attention is all you need: https://arxiv.org/abs/1706.03762\n","\n","[Paper ULMFit] Universal Language Model Fine-tuning for Text Classification: https://arxiv.org/abs/1801.06146"]},{"cell_type":"code","metadata":{"id":"0GM5u7AnBt6z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}