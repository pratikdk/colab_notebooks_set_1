{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"v1_Slim_finetuning_generic_clinical.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNS6zTFze0K1bJMYwbyLmsB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0c4fdcdbe2c94336a8355ba982de057d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_60c173f625ff4bdfb47ff5371760e2bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5fe6022a853417491d038b929fe9b69","IPY_MODEL_c9420a408a524ff3bd3ddb3d7fbefa24"]}},"b9bc0dc7777442ddb1fdd66c3d9a0241":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2dd96c86609744089011d36ffb25e479","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f213760fbd86475194105526c3f5e5f0","IPY_MODEL_50201e3a498748ecaef16eee0d2a01ab"]}},"2dd96c86609744089011d36ffb25e479":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f213760fbd86475194105526c3f5e5f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a0480f4da98c4e42a8e9d0693e38d941","_dom_classes":[],"description":"Prediction: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":7459,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":7459,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c089a040df54c3ab48651304a998981"}},"50201e3a498748ecaef16eee0d2a01ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_91e5073d745b433e965530eea0a2b5ad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 7459/7459 [05:03&lt;00:00, 24.55it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d5f0d67289745408d7f969b9a52d132"}},"a0480f4da98c4e42a8e9d0693e38d941":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c089a040df54c3ab48651304a998981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91e5073d745b433e965530eea0a2b5ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d5f0d67289745408d7f969b9a52d132":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"m-dY2yPa1LRc","colab_type":"text"},"source":["## Imports & Prerequisites"]},{"cell_type":"code","metadata":{"id":"dquNLJkk0ek2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596006895488,"user_tz":-330,"elapsed":927,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"f6eba5f7-ab76-4821-8e69-666ada69885b"},"source":["import os\n","import sys\n","import shutil \n","from distutils.dir_util import copy_tree # Shutil doesn't preserve meta\n","import random\n","import time\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import HTML\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"l0zr0kD-01W4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596006897293,"user_tz":-330,"elapsed":974,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"c08ee561-f832-4e39-a268-567d441a5a0d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aFLJYUh0024w","colab_type":"code","colab":{}},"source":["plt.style.use(\"ggplot\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1NQtDed04TM","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_columns', None)  \n","pd.set_option('display.expand_frame_repr', False)\n","pd.set_option('max_colwidth', None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZwa-vK20-4z","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGETp8j-JcFc","colab_type":"text"},"source":["Install Nvidia Apex library"]},{"cell_type":"code","metadata":{"id":"pLEO4EaPHdQt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595998621106,"user_tz":-330,"elapsed":950,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"3cd6763f-c7ae-46aa-f0a9-05f06b50e78a"},"source":["# %%writefile setup.sh\n","\n","# git clone https://github.com/NVIDIA/apex\n","# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing setup.sh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oywB0bvmGT2i","colab_type":"code","colab":{}},"source":["# !sh setup.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"svYH3lPKJhCS","colab_type":"code","colab":{}},"source":["from apex import amp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uph2XfnZDhOU","colab_type":"text"},"source":["Install HuggingFace Transformers library"]},{"cell_type":"code","metadata":{"id":"MN_1dOMKECwP","colab_type":"code","colab":{}},"source":["#!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KN0ltCWMzfm","colab_type":"text"},"source":["Torch Dataset parent and utility classes"]},{"cell_type":"code","metadata":{"id":"7qtzzWjFNbk3","colab_type":"code","colab":{}},"source":["from torch.utils.data.dataset import Dataset\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfPf9Wlf15HC","colab_type":"text"},"source":["Get interfaces from Transformers"]},{"cell_type":"code","metadata":{"id":"Aht-n3VNL8Pg","colab_type":"code","colab":{}},"source":["from transformers import Trainer, HfArgumentParser, TrainingArguments, EvalPrediction, set_seed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gyD0M9dMaSX","colab_type":"code","colab":{}},"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9A9dSJy1-2V","colab_type":"code","colab":{}},"source":["from transformers import PreTrainedTokenizer, default_data_collator, PreTrainedModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIUoLZzjKM_Q","colab_type":"text"},"source":["Support for Data Classes and Annotations"]},{"cell_type":"code","metadata":{"id":"16yQL7Y9LFmG","colab_type":"code","colab":{}},"source":["from dataclasses import dataclass, field\n","from typing import Dict, Optional, Any\n","from typing import List"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqCaGT2A4Ldn","colab_type":"text"},"source":["## Data download & summary"]},{"cell_type":"markdown","metadata":{"id":"KPGznVmI2e4U","colab_type":"text"},"source":["### Download all tasks"]},{"cell_type":"code","metadata":{"id":"3GziNrPI2oY8","colab_type":"code","colab":{}},"source":["gdrive_tasks_dir = '/content/drive/My Drive/model_csv_files/bert/' # Change this"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h5NUwTp3aaG","colab_type":"code","colab":{}},"source":["task1_filenames = ['task1_pres_elg_train.csv', 'task1_pres_elg_val.csv', 'task1_pres_elg_test.csv']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifSGj9sk3vsQ","colab_type":"code","colab":{}},"source":["task2_filenames = ['task2_cond_elg_train.csv', 'task2_cond_elg_val.csv', 'task2_cond_elg_test.csv']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylsZ3ydj32Hx","colab_type":"code","colab":{}},"source":["task3_filenames = ['task3_cond_intv_train.csv', 'task3_cond_intv_val.csv', 'task3_cond_intv_test.csv']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJl1YhTE5F1e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1596006944605,"user_tz":-330,"elapsed":1783,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"ecbb9e70-3680-4540-c11d-66dfb6dcb41f"},"source":["copy_tree(gdrive_tasks_dir, '/content/data/')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/data/task1_pres_elg_train.csv',\n"," '/content/data/task1_pres_elg_val.csv',\n"," '/content/data/task1_pres_elg_test.csv',\n"," '/content/data/task2_cond_elg_train.csv',\n"," '/content/data/task2_cond_elg_test.csv',\n"," '/content/data/task2_cond_elg_val.csv',\n"," '/content/data/task3_cond_intv_val.csv',\n"," '/content/data/task3_cond_intv_test.csv',\n"," '/content/data/task3_cond_intv_train.csv']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"FpYup1EHdw3I","colab_type":"text"},"source":["## Objective/s\n","Finetune BERT for 3 seperate tasks:\n","- **`Task 1`** Input: Prescription -> Output: Eligibility (Binary)\n","- **`Task 2`** Input: Condition -> Output: Eligibility (Binary)\n","- **`Task 3`** Input: Condition ->  Output: Intervention (Multi class)"]},{"cell_type":"markdown","metadata":{"id":"QX8UG0Z1UnVC","colab_type":"text"},"source":["BERT Finetuning task: Sequence Classification\n","\n","Using [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) and ['BertForSequenceClassification'](https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification) from [huggingface](https://huggingface.co/)"]},{"cell_type":"markdown","metadata":{"id":"X92HeFvQNzN3","colab_type":"text"},"source":["##  Define implemention"]},{"cell_type":"markdown","metadata":{"id":"WCSciUSf4-N-","colab_type":"text"},"source":["### Classes"]},{"cell_type":"code","metadata":{"id":"xMKka3vi3cRc","colab_type":"code","colab":{}},"source":["@dataclass\n","class Features:\n","  input_ids: List[int]\n","  attention_mask: List[int]\n","  label: int"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdmsdFKq-5L7","colab_type":"code","colab":{}},"source":["@dataclass\n","class ModelParameters:\n","  model_name: str = field(\n","      default = None,\n","      metadata = {'help': 'specify pretrained `model name` or `path`'},\n","  )\n","  max_seq_len: Optional[int] = field(\n","      default = None,\n","      metadata = {'help': 'maximum seq len'},\n","  )\n","  dynamic_padding: bool = field(\n","      default = False,\n","      metadata = {'help': 'limit pad size at batch level'},\n","  )\n","  smart_batching: bool = field(\n","      default = False,\n","      metadata = {'help': 'build batch of similar sizes'},\n","  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MdXlM2J4OMI3","colab_type":"code","colab":{}},"source":["# Create a Dataset sub-class and implement necessary methods\n","class SequenceDataset(Dataset):\n","  def __init__(self, tokenizer: PreTrainedTokenizer, pad_to_max_length: bool, max_len: int,\n","               data_df: pd.DataFrame, input_column_name: str, output_column_name: str) -> None:\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.pad_to_max_length = pad_to_max_length\n","    self.data_df: pd.DataFrame = data_df\n","    self.current_row_index: int = 0\n","    self.input_column_name: str = input_column_name\n","    self.output_column_name: str = output_column_name\n","\n","  def encode_sequence(self, sequence: pd.Series) -> Features:\n","    # encode plus returns a dictionary\n","    encode_dict = self.tokenizer.encode_plus(text = sequence[self.input_column_name],\n","                                             add_special_tokens = True,\n","                                             max_length = self.max_len,\n","                                             truncation = True, # Uses max_length to shorten the sequence to 'self.max_len'\n","                                             pad_to_max_length = self.pad_to_max_length,\n","                                             return_token_type_ids = False,\n","                                             return_attention_mask = True,\n","                                             return_overflowing_tokens = False,\n","                                             return_special_tokens_mask = False\n","                                             )\n","    return Features(input_ids = encode_dict['input_ids'],\n","                    attention_mask = encode_dict['attention_mask'],\n","                    label = sequence[self.output_column_name])\n","    \n","  def __getitem__(self, idx) -> Features:\n","    # If dataset is completely parsed, return head to begining of df\n","    if self.current_row_index == self.data_df.shape[0]:\n","      self.current_row_index = 0\n","    # Get the row at current\n","    sequence = self.data_df.loc[self.current_row_index, :]\n","    # Increment the head after extracting sequence at current_row_index\n","    self.current_row_index += 1\n","    # Return an encoded sequence using encode_sequence()\n","    return self.encode_sequence(sequence)\n","\n","  def __len__(self) -> int:\n","    # Return shape of Dataset\n","    return self.data_df.shape[0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OE7QzKPFhFd","colab_type":"code","colab":{}},"source":["def dynamic_data_collator(batch: List[Features]) -> Dict[str, torch.Tensor]:\n","  #batch = default_data_collator(features)\n","  batch_inputs = list()\n","  batch_attention_masks = list()\n","  batch_labels = list()\n","  # Find max size of input in the received batch\n","  max_input_size = max([len(sequence.input_ids) for sequence in batch])\n","  # Iterate through each sequence in batch and apply the padding based on max_input_size\n","  for sequence in batch:\n","    # Pad input ids\n","    batch_inputs += [pad_sequence(sequence.input_ids, max_input_size, 0)]\n","    # Pad Attention mask\n","    batch_attention_masks += [pad_sequence(sequence.attention_mask, max_input_size, 0)]\n","    # Append label as is \n","    batch_labels.append(sequence.label)\n","  \n","  # Return dict with input ids, attention masks, labels for the batch\n","  # Wrap lists in implicit torch long 64-bit integer tensor.\n","  return {\n","      'input_ids': torch.tensor(batch_inputs, dtype=torch.long),\n","      'attention_mask': torch.tensor(batch_attention_masks, dtype=torch.long),\n","      'labels': torch.tensor(batch_labels, dtype=torch.long)\n","  }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GHGlpi0pNgtP","colab_type":"text"},"source":["### Helper Functions"]},{"cell_type":"code","metadata":{"id":"oNwEBsmVYH0d","colab_type":"code","colab":{}},"source":["def convert_args_dict_to_arg_list(arg_dict: Dict[str, Any]) -> List[str]:\n","  args_str_list = []\n","  for arg_op, arg_val in arg_dict.items():\n","    if type(arg_val) == bool:\n","      if arg_val:\n","        args_str_list.append(f\"{arg_op}\")\n","    else:\n","      args_str_list.append(f\"{arg_op}\")\n","      args_str_list.append(f\"{arg_val}\")\n","  return args_str_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTtJrPa7O-M7","colab_type":"code","colab":{}},"source":["def load_data(path: str, X_col_name: str, sort: Optional[bool] = False) -> pd.DataFrame:\n","  # Read csv\n","  data_df = pd.read_csv(path)\n","  if sort: # Sort df to prepare for smart batching\n","    data_df = data_df.loc[data_df[X_col_name].str.split().str.len().rename(\"count\").sort_values().index]\n","  return data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUcCe5qEL56u","colab_type":"code","colab":{}},"source":["def organize_data(data_df: pd.DataFrame, batch_size: int, smart_batching: bool, forTrain: bool = True) -> pd.DataFrame:\n","  # Compute approx number of batches\n","  n_batches = np.ceil(data_df.shape[0]/batch_size).astype(int)\n","  if forTrain:\n","    if smart_batching: # For Training with Smart Batching\n","      data_df = np.array_split(data_df, n_batches)\n","      np.random.shuffle(data_df)\n","      data_df = pd.concat(data_df, axis=0)\n","    else: # For Training with Random Batching\n","      data_df = np.array_split(data_df.sample(frac=1), n_batches)\n","      data_df = pd.concat(data_df, axis=0)\n","  return data_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVm4Lb3SbWCF","colab_type":"code","colab":{}},"source":["def load_pretrained_model(pretrained_model_name_or_path: str,\n","                          use_cuda: bool,\n","                          mixed_precision: bool,\n","                          num_labels: int) -> PreTrainedModel:\n","  # Download model config\n","  model_config = AutoConfig.from_pretrained(\n","      pretrained_model_name_or_path = pretrained_model_name_or_path,\n","      num_labels = num_labels)\n","  # Download model using model config\n","  model = AutoModelForSequenceClassification.from_pretrained(\n","      pretrained_model_name_or_path = pretrained_model_name_or_path,\n","      config = model_config\n","  )\n","  # Utilize GPU if specified and available\n","  if use_cuda and torch.cuda.is_available():\n","    # Specify and select device as GPU\n","    device = torch.device('cuda')\n","    # Transfer model execution to GPU device\n","    model.to(device)\n","\n","  # Utilize Mixed precision if specified\n","  if mixed_precision:\n","    try:\n","      model = amp.initialize(model, opt_level='O1')\n","    except:\n","      raise ValueError('Trouble initializing `mixed precision` on the initialized model')\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTFflW4-7VNO","colab_type":"code","colab":{}},"source":["def pad_sequence(seq: List[int], max_input_size: int, pad_value: int) -> List[int]:\n","  return seq + (max_input_size - len(seq)) * [pad_value]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83LNIFRc2nme","colab_type":"code","colab":{}},"source":["def compute_acc_metric(p: EvalPrediction) -> Dict:\n","    preds = np.argmax(p.predictions, axis=1)\n","    return {\"acc\": (preds == p.label_ids).mean()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-u_PFz_xNlbn","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"WglYGQUCL1ZM","colab_type":"code","colab":{}},"source":["def train(finetuning_args_dict: Dict[str, Any],\n","          train_file_path: str, X_col_name: str, y_col_name: str, num_labels: int,\n","          validation_file_path: Optional[str] = None,):\n","  \n","  # Trainer needs Training Arguments to access all the points of customization during training.\n","  # HfArgumentParser is subclass of ArgumentParser, uses type hints on dataclasses to generate arguments.\n","  parser = HfArgumentParser((TrainingArguments, ModelParameters))\n","  trainer_args, model_args = parser.parse_args_into_dataclasses(convert_args_dict_to_arg_list(finetuning_args_dict))\n","  \n","  # If validating, check if path is supplied\n","  if validation_file_path is None and trainer_args.evaluate_during_training == True:\n","    raise ValueError(\"'validation_file_path' must be a supplied, when 'evaluate_during_training == True'.\")\n","  # Check if both data exists in path\n","  if not os.path.exists(train_file_path) or not os.path.exists(validation_file_path):\n","    raise ValueError(\"Please make sure specified data files exist at their location and rerun.\")\n","  \n","  # Load train data\n","  train_data_df = load_data(train_file_path, X_col_name, model_args.smart_batching)\n","  \n","  # Auto load respective tokenizer (Eg: name = bert-base-uncased)\n","  tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_args.model_name)\n","  \n","  # Organize Train data\n","  organized_train_df = organize_data(data_df = train_data_df,\n","                                     batch_size = trainer_args.per_gpu_train_batch_size,\n","                                     smart_batching = model_args.smart_batching,\n","                                     forTrain = True)\n","  \n","  # Construct Train Dataset\n","  train_dataset = SequenceDataset(tokenizer = tokenizer,\n","                                  max_len = model_args.max_seq_len,\n","                                  pad_to_max_length = not model_args.dynamic_padding,\n","                                  data_df = organized_train_df,\n","                                  input_column_name = X_col_name,\n","                                  output_column_name = y_col_name)\n","\n","  # Download and load pretrained model\n","  model = load_pretrained_model(pretrained_model_name_or_path = model_args.model_name,\n","                                use_cuda = True,\n","                                mixed_precision = trainer_args.fp16,\n","                                num_labels = num_labels)\n","\n","  \n","  validation_dataset = None\n","  # We validate (if requested)\n","  if trainer_args.evaluate_during_training == True:\n","    # Load Validation dataset\n","    val_data_df = load_data(validation_file_path, X_col_name, model_args.smart_batching)\n","    # Organize Validation batches\n","    organized_val_df = organize_data(data_df=val_data_df,\n","                                     batch_size=trainer_args.per_gpu_train_batch_size,\n","                                     smart_batching = model_args.smart_batching,\n","                                     forTrain=False)\n","    # Construct Validation Dataset\n","    validation_dataset = SequenceDataset(tokenizer = tokenizer,\n","                                         max_len = model_args.max_seq_len,\n","                                         pad_to_max_length = not model_args.dynamic_padding,\n","                                         data_df = organized_val_df,\n","                                         input_column_name = X_col_name,\n","                                         output_column_name = y_col_name)\n","  \n","  # Define and Intialize a Trainer \n","  # https://huggingface.co/transformers/main_classes/trainer.html\n","  trainer = Trainer(\n","      model = model,\n","      args = trainer_args,\n","      train_dataset = train_dataset,\n","      eval_dataset = validation_dataset,\n","      data_collator = dynamic_data_collator,\n","      compute_metrics = compute_acc_metric,\n","      tb_writer = SummaryWriter(log_dir='logs', flush_secs=10)\n","  )\n","\n","\n","  tick = time.time() # Trainer start time\n","  trainer.train()\n","  tock = time.time() # Trainer end time\n","  print('Training Complete.')\n","  print(f\"Total training time: {time.strftime('%H:%M:%S', time.gmtime(tock-tick))}\")\n","\n","  print('Saving Model')\n","  trainer.save_model()\n","\n","  eval_result = None\n","  if trainer_args.evaluate_during_training == True:\n","    print('Evaluating Model')\n","    eval_result = trainer.evaluate()\n","    print('Evaluation Complete.')\n","\n","  return trainer, eval_result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksbdAhr1Nq_U","colab_type":"text"},"source":["### Predict"]},{"cell_type":"code","metadata":{"id":"Q5ehwyTMZC-L","colab_type":"code","colab":{}},"source":["def predict(finetuning_args_dict: Dict[str, Any], test_file_path: str, X_col_name: str, y_col_name: str, num_labels: int, trained_trainer: Trainer):\n","  # Arguments \n","  parser = HfArgumentParser((TrainingArguments, ModelParameters))\n","  trainer_args, model_args = parser.parse_args_into_dataclasses(convert_args_dict_to_arg_list(finetuning_args_dict))\n","\n","  # Load test data\n","  test_data_df = load_data(test_file_path, X_col_name, model_args.smart_batching)\n","\n","  # Auto load respective tokenizer (Eg: name = bert-base-uncased)\n","  tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = model_args.model_name)\n","\n","  # Organize Testing batches\n","  organized_test_df = organize_data(data_df=test_data_df,\n","                                    batch_size=trainer_args.per_gpu_train_batch_size,\n","                                    smart_batching = model_args.smart_batching,\n","                                    forTrain=False)\n","  \n","\n","  # Construct Test Dataset\n","  test_dataset = SequenceDataset(tokenizer = tokenizer,\n","                                max_len = model_args.max_seq_len,\n","                                pad_to_max_length = not model_args.dynamic_padding,\n","                                data_df = organized_test_df,\n","                                input_column_name = X_col_name,\n","                                output_column_name = y_col_name)\n","  print('Testing Model')\n","  predictions, label_ids, metrics = trained_trainer.predict(test_dataset)\n","  print('Testing Complete.')\n","\n","  return predictions, label_ids, metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8qr2XqSTtBa","colab_type":"text"},"source":["## Finetune"]},{"cell_type":"code","metadata":{"id":"hbo2TVmsUyKO","colab_type":"code","colab":{}},"source":["seed_value = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mEidK0z-U_Kk","colab_type":"code","colab":{}},"source":["set_seed(seed_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-y8SC7hVRuz","colab_type":"code","colab":{}},"source":["model_save_root_directory = \"model_save\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AIv8vyaK9hBC","colab_type":"text"},"source":["Name or the path of the pretrained model"]},{"cell_type":"code","metadata":{"id":"AlKfhgOm8pcX","colab_type":"code","colab":{}},"source":["# Name or the path of the pretrained model\n","model_base_name = 'bert-base-uncased'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0yVOvzINYY6Q","colab_type":"text"},"source":["Token length Analysis\n","\n","[https://colab.research.google.com/drive/1QSVCtQlDnhW_ico8rgULE2kETyABiGN_?usp=sharing](https://colab.research.google.com/drive/1QSVCtQlDnhW_ico8rgULE2kETyABiGN_?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"FdyY2e6YWV3k","colab_type":"code","colab":{}},"source":["# Maximum token length for prescription\n","pres_max_token_len = 75"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"giIz7EaOWetL","colab_type":"code","colab":{}},"source":["# Maximum token length for condition\n","cond_max_token_len = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4kqlkxIXSluA","colab_type":"text"},"source":["Some utility functions for colab"]},{"cell_type":"code","metadata":{"id":"wcKuIRcpUFHa","colab_type":"code","colab":{}},"source":["def del_directory_tree(folder_name):\n","  shutil.rmtree(f\"{folder_name}\")\n","  if not os.path.exists(folder_name):\n","    print(f\"{folder_name} tree deleted.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5EP0oJeXUL4-","colab_type":"code","colab":{}},"source":["def create_directory_tree(tree_path):\n","  if not os.path.exists(tree_path):\n","    os.makedirs(tree_path)\n","  if os.path.exists(tree_path):\n","    print(f\"{tree_path} tree/path created.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5V7CFBYyjhb","colab_type":"code","colab":{}},"source":["def save_to_drive(from_dir_path='model_save', to_path=\"/content/drive/My Drive/trained_model_files\", folder_prefix=\"bert\", from_is_file=False):\n","  print(f'Saving model on google drive...')\n","  from_dir = f\"{from_dir_path}\"\n","  to_dir = f\"{to_path}/{folder_prefix}_{from_dir}\"\n","  try:\n","    if from_is_file:\n","      to_dir = f\"{to_path}/\"\n","      if not os.path.exists(to_dir):\n","        os.makedirs(to_dir)\n","      shutil.copy2(from_dir, to_dir)\n","    else:\n","      if not os.path.exists(to_dir):\n","        os.makedirs(to_dir)\n","      copy_tree(from_dir, to_dir)\n","    print(f'Model saved.')\n","  except:\n","    print(f\"Copy to Google drive failed: {sys.exc_info()[-2:]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tk174r3tXs7h","colab_type":"code","colab":{}},"source":["#del_directory_tree('logs')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WQEl6OYXtEj","colab_type":"text"},"source":["### Task 1: Prescription -> Eligibility"]},{"cell_type":"code","metadata":{"id":"Atl0YWq9YBl2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596007009501,"user_tz":-330,"elapsed":861,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"bbd675ab-30a8-42e4-82e6-991c1f78a808"},"source":["# Task 1 file names\n","task1_filenames"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['task1_pres_elg_train.csv',\n"," 'task1_pres_elg_val.csv',\n"," 'task1_pres_elg_test.csv']"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"dqKr2LblYxx2","colab_type":"code","colab":{}},"source":["task1_save_directory_name = \"task1_pres_elg_mixed_dynamic_smart_batch_16_seed_128\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gaeRYBXMYF2k","colab_type":"code","colab":{}},"source":["task1_finetuning_args = {\n","\t'--output_dir': f'./{model_save_root_directory}/{task1_save_directory_name}',\n","\t'--overwrite_output_dir': True,\n","\t'--save_steps': 0,\n","\t'--seed': seed_value,\n","\t'--num_train_epochs': 1,\n","\t'--learning_rate': 5e-5,\n","\t'--per_gpu_train_batch_size': 16,\n","\t'--gradient_accumulation_steps': 1,\n","\t'--per_gpu_eval_batch_size': 16,\n","  '--evaluate_during_training': False, # Enable if logs are needed\n","  '--max_seq_len': pres_max_token_len, # 75 here for prescription\n","\t'--dynamic_padding': True,\n","\t'--smart_batching': True,\n","\t'--fp16': True,\n","\t'--model_name': model_base_name\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PfQUCqbOZLbW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596007015627,"user_tz":-330,"elapsed":933,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"7244f6c1-37c6-4ef1-a8ea-3ccf024f8723"},"source":["# Create model output dir\n","create_directory_tree(task1_finetuning_args['--output_dir'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["./model_save/task1_pres_elg_mixed_dynamic_smart_batch_16_seed_128 tree/path created.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dB1o9vZaZuFf","colab_type":"text"},"source":["Train"]},{"cell_type":"code","metadata":{"id":"7DefrkCwZztS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["0c4fdcdbe2c94336a8355ba982de057d","60c173f625ff4bdfb47ff5371760e2bb","d5fe6022a853417491d038b929fe9b69","c9420a408a524ff3bd3ddb3d7fbefa24","9de28ccff215464fa24d9f80c9f07ab7"]},"outputId":"0af25f12-eb18-49d0-9b7e-c1d28002a81e"},"source":["task1_trainer, task1_eval_result = train(finetuning_args_dict = task1_finetuning_args,\n","                                         train_file_path = f'/content/data/{task1_filenames[0]}',\n","                                         X_col_name = 'prescription',\n","                                         y_col_name = 'label',\n","                                         num_labels = 2,\n","                                         validation_file_path = f'/content/data/{task1_filenames[1]}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"],"name":"stderr"},{"output_type":"stream","text":["Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O1\n","cast_model_type        : None\n","patch_torch_functions  : True\n","keep_batchnorm_fp32    : None\n","master_weights         : None\n","loss_scale             : dynamic\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c4fdcdbe2c94336a8355ba982de057d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='iâ€¦"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9de28ccff215464fa24d9f80c9f07ab7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Iteration', max=26853.0, style=ProgressStyle(description_â€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2jcN39zFzsww","colab_type":"text"},"source":["Saving model to Google Drive"]},{"cell_type":"code","metadata":{"id":"5MkiNT7Czxf1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596013415166,"user_tz":-330,"elapsed":3364,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"c8a54f62-3afb-48e6-aea6-2ee3942fa24a"},"source":["save_to_drive(from_dir_path='model_save', to_path=\"/content/drive/My Drive/trained_model_files\", folder_prefix=\"bert\", from_is_file=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model on google drive...\n","Model saved.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z23i_rbN0ahF","colab_type":"text"},"source":["Saving logs"]},{"cell_type":"code","metadata":{"id":"aJSlpQ8e0gMw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596013771426,"user_tz":-330,"elapsed":1055,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"94b83ce3-2585-492e-9523-cd37a2034d67"},"source":["save_to_drive(from_dir_path='logs', to_path=\"/content/drive/My Drive/trained_model_files\", folder_prefix=\"Logs_bert\", from_is_file=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving model on google drive...\n","Model saved.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xcvM3vPIbQGA","colab_type":"text"},"source":["Predict"]},{"cell_type":"code","metadata":{"id":"Q45bquuPbm1N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["b9bc0dc7777442ddb1fdd66c3d9a0241","2dd96c86609744089011d36ffb25e479","f213760fbd86475194105526c3f5e5f0","50201e3a498748ecaef16eee0d2a01ab","a0480f4da98c4e42a8e9d0693e38d941","0c089a040df54c3ab48651304a998981","91e5073d745b433e965530eea0a2b5ad","2d5f0d67289745408d7f969b9a52d132"]},"executionInfo":{"status":"ok","timestamp":1596013769194,"user_tz":-330,"elapsed":306796,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"0790cf65-6b19-49a5-d4fe-ca89c009d00b"},"source":["predictions, label_ids, metrics = predict(finetuning_args_dict = task1_finetuning_args,\n","                                          test_file_path = f'/content/data/{task1_filenames[2]}',\n","                                          X_col_name = 'prescription',\n","                                          y_col_name = 'label',\n","                                          num_labels = 2,\n","                                          trained_trainer = task1_trainer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"],"name":"stderr"},{"output_type":"stream","text":["Testing Model\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9bc0dc7777442ddb1fdd66c3d9a0241","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Prediction', max=7459.0, style=ProgressStyle(description_â€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Testing Complete.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1TVqy9ChGLaH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"status":"error","timestamp":1596023073774,"user_tz":-330,"elapsed":1228,"user":{"displayName":"Pratik Deoolwadikar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjm_ROtGV1QqOCXXiZ8e-3uKpyPdd3dXpyE4NeJww=s64","userId":"12232666687479412064"}},"outputId":"c32d42a0-e1a8-4f3c-b62f-ba612b802b4b"},"source":["metrics # Fuck rerun "],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-71cacc06bb91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"tLUllXVtt-RV","colab_type":"text"},"source":["Resources I used:"]},{"cell_type":"markdown","metadata":{"id":"4t4Mwtw84z59","colab_type":"text"},"source":["**[Slow approach]**\n","**Native [Pytorch + Huggingface] approach (2hrs/epoch)**:: based approach**(4.5hrs)** + gradient accumulation(-0) + mixed precision**(-2.5hrs)**\n","https://colab.research.google.com/drive/1TJmhr-n9_Ynrb6eusAim-7DmuqY34OcC?usp=sharing\n","\n","**Resources:**\n","**(Google Tensorflow)** https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb#scrollTo=lXsXev5MNr20\n","**(Glue)** https://mccormickml.com/2019/11/05/GLUE/\n","**(Finetuning BERT Native)** https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n","**(Barebones Word and Sentence Embedding extraction strategy)** http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n","**(Bert Concept)** https://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/\n","**(Bio_Discharge BERT)** https://huggingface.co/emilyalsentzer/Bio_Discharge_Summary_BERT\n","\n","**Model execution improvement techniques:**\n","**Grad Accumulation**\n","- https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce\n","- https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa\n","- https://towardsdatascience.com/how-to-easily-use-gradient-accumulation-in-keras-models-fa02c0342b60\n","- (Grad Accum + Distributed) https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\n","\n","**Grad Accumulation + Mixed Precision + Dynamic Batching + Smart Batching**\n","- (Deep Explanations) https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n","- (Performance overview & visualizations) https://app.wandb.ai/pommedeterresautee/speed_training/reports/Train-HuggingFace-models-twice-as-fast-with-dynamic-padding-and-uniform-length-batching--VmlldzoxMDgzOTI\n","- (Lightning Library approaches) https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565\n","(Brief implementation overview): https://huggingface.co/transformers/training.html\n","- (Trainer class overview): https://huggingface.co/transformers/main_classes/trainer.html\n","\n","**Nvidia Apex Presentation for Mixed Precision comparison:** https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9998-automatic-mixed-precision-in-pytorch.pdf\n","\n","**[Fast approach]**\n","**Trainer [Huggingface] approach (1hr/epoch)**:: base approch (unknown) + gradient accumulation + dynamic padding + smart batching + mixed precision\n","https://colab.research.google.com/drive/1tWk9BFsdANr25yXANdalPP3wXhUBWRk3?usp=sharing"]},{"cell_type":"code","metadata":{"id":"NGzNO14dTVM-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUmeYRUtt8B8","colab_type":"code","colab":{}},"source":["d=[]\n","while(1):\n","  d.append('1')"],"execution_count":null,"outputs":[]}]}